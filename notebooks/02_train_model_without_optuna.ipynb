{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSDK_7Dj3YH9",
        "outputId": "1ddde303-c2e8-4612-8100-36cf689ffa7f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost lightgbm \"mlflow<3\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "quIhilm65orD",
        "outputId": "daf0111b-5a1f-4c55-95e2-d46b5143ee37"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Collecting mlflow<3\n",
            "  Downloading mlflow-2.22.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.3)\n",
            "Collecting mlflow-skinny==2.22.4 (from mlflow<3)\n",
            "  Downloading mlflow_skinny-2.22.4-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (3.1.2)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (3.1.6)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (1.17.2)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow<3)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow<3)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow<3)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (3.10)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (3.10.0)\n",
            "Requirement already satisfied: pandas!=2.3.0,<3 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<20,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (1.6.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (2.0.44)\n",
            "Collecting cachetools<6,>=5.0.0 (from mlflow-skinny==2.22.4->mlflow<3)\n",
            "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (8.3.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (3.1.2)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.22.4->mlflow<3)\n",
            "  Downloading databricks_sdk-0.74.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (0.118.3)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (1.37.0)\n",
            "Collecting packaging<25 (from mlflow-skinny==2.22.4->mlflow<3)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (2.32.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (0.5.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (0.38.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow<3) (1.3.10)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow<3) (2.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow<3) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow<3) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow<3) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow<3) (3.1.4)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow<3)\n",
            "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow<3)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow<3) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow<3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow<3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow<3) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow<3) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow<3) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow<3) (3.2.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.3.0,<3->mlflow<3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.3.0,<3->mlflow<3) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow<3) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow<3) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow<3) (3.3.0)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.4->mlflow<3) (2.43.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==2.22.4->mlflow<3) (0.48.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.22.4->mlflow<3) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.22.4->mlflow<3) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.22.4->mlflow<3) (0.58b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.4->mlflow<3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.4->mlflow<3) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.4->mlflow<3) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow<3) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.4->mlflow<3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.4->mlflow<3) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.4->mlflow<3) (2025.11.12)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1->mlflow-skinny==2.22.4->mlflow<3) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.22.4->mlflow<3) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.4->mlflow<3) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.4->mlflow<3) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.49.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.4->mlflow<3) (4.12.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.4->mlflow<3) (0.6.1)\n",
            "Downloading mlflow-2.22.4-py3-none-any.whl (29.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.22.4-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Downloading databricks_sdk-0.74.0-py3-none-any.whl (764 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.2/764.2 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: packaging, graphql-core, cachetools, gunicorn, graphql-relay, docker, graphene, databricks-sdk, mlflow-skinny, mlflow\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 6.2.2\n",
            "    Uninstalling cachetools-6.2.2:\n",
            "      Successfully uninstalled cachetools-6.2.2\n",
            "Successfully installed cachetools-5.5.2 databricks-sdk-0.74.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-2.22.4 mlflow-skinny-2.22.4 packaging-24.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              },
              "id": "c153986371424ee38bdb3d3e2c51ed94"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_folder = \"/content/drive/MyDrive/Colab Notebooks/housing_fall2025\"\n",
        "%cd \"{base_folder}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRFqzWx-55CJ",
        "outputId": "c29141ec-c5ad-4588-c0df-e2cfe379a509"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/housing_fall2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "m1pqkWQ53TFh",
        "outputId": "070a405b-7ffb-4945-e9ce-723ddf5dda26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   block_id  longitude  latitude  housing_median_age  total_rooms  \\\n",
              "0         0    -122.23     37.88                41.0          880   \n",
              "1         1    -122.22     37.86                21.0         7099   \n",
              "2         2    -122.24     37.85                52.0         1467   \n",
              "3         3    -122.25     37.85                52.0         1274   \n",
              "4         4    -122.25     37.85                52.0         1627   \n",
              "\n",
              "   total_bedrooms  population  households  median_income  median_house_value  \\\n",
              "0           129.0         322         126         8.3252            452600.0   \n",
              "1          1106.0        2401        1138         8.3014            358500.0   \n",
              "2           190.0         496         177         7.2574            352100.0   \n",
              "3           235.0         558         219         5.6431            341300.0   \n",
              "4           280.0         565         259         3.8462            342200.0   \n",
              "\n",
              "  ocean_proximity  \n",
              "0        NEAR BAY  \n",
              "1        NEAR BAY  \n",
              "2        NEAR BAY  \n",
              "3        NEAR BAY  \n",
              "4        NEAR BAY  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-384e642c-d7fb-4066-b11c-e6364738b4e9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>block_id</th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "      <th>ocean_proximity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-122.23</td>\n",
              "      <td>37.88</td>\n",
              "      <td>41.0</td>\n",
              "      <td>880</td>\n",
              "      <td>129.0</td>\n",
              "      <td>322</td>\n",
              "      <td>126</td>\n",
              "      <td>8.3252</td>\n",
              "      <td>452600.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-122.22</td>\n",
              "      <td>37.86</td>\n",
              "      <td>21.0</td>\n",
              "      <td>7099</td>\n",
              "      <td>1106.0</td>\n",
              "      <td>2401</td>\n",
              "      <td>1138</td>\n",
              "      <td>8.3014</td>\n",
              "      <td>358500.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-122.24</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1467</td>\n",
              "      <td>190.0</td>\n",
              "      <td>496</td>\n",
              "      <td>177</td>\n",
              "      <td>7.2574</td>\n",
              "      <td>352100.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-122.25</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1274</td>\n",
              "      <td>235.0</td>\n",
              "      <td>558</td>\n",
              "      <td>219</td>\n",
              "      <td>5.6431</td>\n",
              "      <td>341300.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-122.25</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1627</td>\n",
              "      <td>280.0</td>\n",
              "      <td>565</td>\n",
              "      <td>259</td>\n",
              "      <td>3.8462</td>\n",
              "      <td>342200.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-384e642c-d7fb-4066-b11c-e6364738b4e9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-384e642c-d7fb-4066-b11c-e6364738b4e9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-384e642c-d7fb-4066-b11c-e6364738b4e9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "conn = sqlite3.connect(f\"{base_folder}/data/housing.db\")\n",
        "housing = pd.read_sql_query(\n",
        "    \"\"\"\n",
        "    SELECT\n",
        "        b.block_id,\n",
        "        b.longitude,\n",
        "        b.latitude,\n",
        "        s.housing_median_age,\n",
        "        s.total_rooms,\n",
        "        s.total_bedrooms,\n",
        "        s.population,\n",
        "        s.households,\n",
        "        s.median_income,\n",
        "        s.median_house_value,\n",
        "        op.name AS ocean_proximity\n",
        "    FROM block AS b\n",
        "    JOIN block_housing_stats AS s\n",
        "        ON s.block_id = b.block_id\n",
        "    JOIN ocean_proximity AS op\n",
        "        ON op.ocean_proximity_id = b.ocean_proximity_id\n",
        "    ORDER BY b.block_id\n",
        "    \"\"\",\n",
        "    conn,\n",
        ")\n",
        "conn.close()\n",
        "\n",
        "housing.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FULL PIPELINE:\n",
        "# - Build preprocessing\n",
        "# - Stratified train/test split\n",
        "# - Train & log 4 models WITHOUT PCA (Ridge, HGB, XGBoost, LightGBM)\n",
        "# - Train & log 4 models WITH PCA (preprocessing + PCA(0.95) + model)\n",
        "# - Pick GLOBAL best among 8 models by Test MAE\n",
        "# - Save, load, and compare the global best model\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "import mlflow\n",
        "from mlflow.models import infer_signature\n",
        "import joblib\n",
        "\n",
        "# Import shared components\n",
        "from housing_pipeline import (\n",
        "    build_preprocessing,\n",
        "    make_estimator_for_name,\n",
        ")\n",
        "\n",
        "start_time = time.monotonic()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: Build Full ML Preprocessing Pipeline\n",
        "# =============================================================================\n",
        "\n",
        "preprocessing = build_preprocessing()\n",
        "print(\"✓ STEP 1: Preprocessing pipeline created.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: Split Data into Stratified Train and Test Sets\n",
        "# =============================================================================\n",
        "\n",
        "housing[\"income_cat\"] = pd.cut(\n",
        "    housing[\"median_income\"],\n",
        "    bins=[0, 1.5, 3.0, 4.5, 6, np.inf],\n",
        "    labels=[1, 2, 3, 4, 5],\n",
        ")\n",
        "\n",
        "train_set, test_set = train_test_split(\n",
        "    housing,\n",
        "    test_size=0.20,\n",
        "    stratify=housing[\"income_cat\"],\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "for df in (train_set, test_set):\n",
        "    df.drop(\"income_cat\", axis=1, inplace=True)\n",
        "\n",
        "X_train = train_set.drop([\"block_id\", \"median_house_value\"], axis=1).copy()\n",
        "y_train = train_set[\"median_house_value\"].copy()\n",
        "\n",
        "X_test = test_set.drop([\"block_id\", \"median_house_value\"], axis=1).copy()\n",
        "y_test = test_set[\"median_house_value\"].copy()\n",
        "\n",
        "print(f\"✓ STEP 2: Stratified split done. Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: Define 4 Model Pipelines (WITHOUT PCA)\n",
        "# =============================================================================\n",
        "\n",
        "models = {}\n",
        "for name in [\"ridge\", \"histgradientboosting\", \"xgboost\", \"lightgbm\"]:\n",
        "    est = make_estimator_for_name(name)\n",
        "    models[name] = make_pipeline(preprocessing, est)\n",
        "\n",
        "print(\"✓ STEP 3: 4 baseline model pipelines defined.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: Configure MLflow (e.g., Dagshub) via .env\n",
        "# =============================================================================\n",
        "\n",
        "load_dotenv(\n",
        "    dotenv_path=\"/content/gdrive/MyDrive/Colab Notebooks/housing_fall2025/notebooks/.env\",\n",
        "    override=True\n",
        ")\n",
        "\n",
        "MLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
        "MLFLOW_TRACKING_USERNAME = os.getenv(\"MLFLOW_TRACKING_USERNAME\")\n",
        "MLFLOW_TRACKING_PASSWORD = os.getenv(\"MLFLOW_TRACKING_PASSWORD\")\n",
        "\n",
        "if MLFLOW_TRACKING_USERNAME:\n",
        "    os.environ[\"MLFLOW_TRACKING_USERNAME\"] = MLFLOW_TRACKING_USERNAME\n",
        "if MLFLOW_TRACKING_PASSWORD:\n",
        "    os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = MLFLOW_TRACKING_PASSWORD\n",
        "\n",
        "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "mlflow.set_experiment(\"median_house_pricing_multi_model\")\n",
        "\n",
        "print(\"✓ STEP 4: MLflow configured.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: Train, Evaluate, and Log 4 Baseline Models (NO PCA)\n",
        "# =============================================================================\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, pipeline in models.items():\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(f\"Training baseline model: {name}\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "\n",
        "    # Compute CV MAE before fitting on full training set\n",
        "    cv_scores = cross_val_score(\n",
        "        pipeline, X_train, y_train,\n",
        "        cv=3, scoring=\"neg_mean_absolute_error\", n_jobs=-1\n",
        "    )\n",
        "    cv_mae = -cv_scores.mean()\n",
        "    print(f\"{name} (no PCA) CV MAE: ${cv_mae:,.2f}\")\n",
        "\n",
        "    # Fit on full training set\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    test_mae = mean_absolute_error(y_test, y_pred)\n",
        "    print(f\"{name} (no PCA) Test MAE: ${test_mae:,.2f}\")\n",
        "\n",
        "    results[name] = {\"pipeline\": pipeline, \"test_mae\": test_mae, \"cv_mae\": cv_mae}\n",
        "\n",
        "    with mlflow.start_run(run_name=f\"{name}_baseline\"):\n",
        "        mlflow.log_param(\"model_family\", name)\n",
        "        mlflow.log_param(\"uses_pca\", False)\n",
        "\n",
        "        est_step_name = list(pipeline.named_steps.keys())[-1]\n",
        "        est = pipeline.named_steps[est_step_name]\n",
        "        est_params = {f\"{est_step_name}__{k}\": v for k, v in est.get_params().items()}\n",
        "        mlflow.log_params(est_params)\n",
        "\n",
        "        mlflow.log_metric(\"cv_MAE\", cv_mae)\n",
        "        mlflow.log_metric(\"test_MAE\", test_mae)\n",
        "\n",
        "        signature = infer_signature(X_train, pipeline.predict(X_train))\n",
        "        mlflow.sklearn.log_model(\n",
        "            sk_model=pipeline,\n",
        "            artifact_path=\"housing_model\",\n",
        "            signature=signature,\n",
        "            input_example=X_train,\n",
        "            registered_model_name=f\"{name}_pipeline\",\n",
        "        )\n",
        "\n",
        "print(\"\\n✓ STEP 5: All 4 baseline models trained and logged.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 7: Train, Evaluate, and Log PCA Versions of ALL 4 Models\n",
        "# =============================================================================\n",
        "\n",
        "pca_results = {}\n",
        "\n",
        "for name in models.keys():\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Training PCA-augmented model: {name}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    est = make_estimator_for_name(name)\n",
        "\n",
        "    pca_pipeline = make_pipeline(\n",
        "        preprocessing,\n",
        "        PCA(n_components=0.95),\n",
        "        est,\n",
        "    )\n",
        "\n",
        "    # Compute CV MAE before fitting on full training set\n",
        "    cv_scores_pca = cross_val_score(\n",
        "        pca_pipeline, X_train, y_train,\n",
        "        cv=3, scoring=\"neg_mean_absolute_error\", n_jobs=-1\n",
        "    )\n",
        "    cv_mae_pca = -cv_scores_pca.mean()\n",
        "    print(f\"{name}_with_pca CV MAE: ${cv_mae_pca:,.2f}\")\n",
        "\n",
        "    # Fit on full training set\n",
        "    pca_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    y_pred_pca = pca_pipeline.predict(X_test)\n",
        "    test_mae_pca = mean_absolute_error(y_test, y_pred_pca)\n",
        "\n",
        "    model_key = f\"{name}_with_pca\"\n",
        "    pca_results[model_key] = {\n",
        "        \"pipeline\": pca_pipeline,\n",
        "        \"test_mae\": test_mae_pca,\n",
        "        \"cv_mae\": cv_mae_pca,\n",
        "    }\n",
        "\n",
        "    print(f\"{model_key} Test MAE: ${test_mae_pca:,.2f}\")\n",
        "\n",
        "    with mlflow.start_run(run_name=model_key):\n",
        "        mlflow.log_param(\"model_family\", name)\n",
        "        mlflow.log_param(\"uses_pca\", True)\n",
        "\n",
        "        est_step_name = list(pca_pipeline.named_steps.keys())[-1]\n",
        "        est_step = pca_pipeline.named_steps[est_step_name]\n",
        "        est_params = {f\"{est_step_name}__{k}\": v for k, v in est_step.get_params().items()}\n",
        "        mlflow.log_params(est_params)\n",
        "\n",
        "        pca_step = pca_pipeline.named_steps[\"pca\"]\n",
        "        mlflow.log_param(\"pca__n_components\", pca_step.n_components)\n",
        "\n",
        "        mlflow.log_metric(\"cv_MAE\", cv_mae_pca)\n",
        "        mlflow.log_metric(\"test_MAE\", test_mae_pca)\n",
        "\n",
        "        signature_pca = infer_signature(X_train, pca_pipeline.predict(X_train))\n",
        "        mlflow.sklearn.log_model(\n",
        "            sk_model=pca_pipeline,\n",
        "            artifact_path=\"housing_model_with_pca\",\n",
        "            signature=signature_pca,\n",
        "            input_example=X_train,\n",
        "            registered_model_name=f\"{name}_pipeline_with_pca\",\n",
        "        )\n",
        "\n",
        "print(\"\\n✓ STEP 7: All 4 PCA models trained and logged.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 8: Choose GLOBAL Best Model (with or without PCA)\n",
        "# =============================================================================\n",
        "\n",
        "all_results = {}\n",
        "all_results.update(results)\n",
        "all_results.update(pca_results)\n",
        "\n",
        "global_best_name = min(all_results, key=lambda k: all_results[k][\"test_mae\"])\n",
        "global_best_mae = all_results[global_best_name][\"test_mae\"]\n",
        "global_best_cv_mae = all_results[global_best_name][\"cv_mae\"]\n",
        "global_best_pipeline = all_results[global_best_name][\"pipeline\"]\n",
        "\n",
        "uses_pca = \"with_pca\" in global_best_name\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GLOBAL BEST MODEL (ACROSS 8 CANDIDATES)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Global best model key: {global_best_name}\")\n",
        "print(f\"Global best CV MAE:    ${global_best_cv_mae:,.2f}\")\n",
        "print(f\"Global best Test MAE:  ${global_best_mae:,.2f}\")\n",
        "print(f\"Uses PCA:               {uses_pca}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 9: Save, Load, and Compare the GLOBAL Best Model\n",
        "# =============================================================================\n",
        "\n",
        "def save_model(model, filename=\"global_best_model.pkl\"):\n",
        "    joblib.dump(model, filename)\n",
        "    print(f\"✓ Model saved to {filename}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"Saving and reloading GLOBAL best model...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "save_model(global_best_pipeline, filename=f\"{base_folder}/models/global_best_model.pkl\")\n",
        "\n",
        "print(\"\\nDone:\")\n",
        "print(f\"- GLOBAL best model key: {global_best_name}\")\n",
        "print(f\"- GLOBAL best CV MAE:    ${global_best_cv_mae:,.2f}\")\n",
        "print(f\"- GLOBAL best Test MAE:  ${global_best_mae:,.2f}\")\n",
        "\n",
        "end_time = time.monotonic()\n",
        "elapsed_time = end_time - start_time\n",
        "minutes = int(elapsed_time // 60)\n",
        "seconds = elapsed_time % 60\n",
        "print(f\"Elapsed time: {minutes} minutes and {seconds:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTqNkkbR33iG",
        "outputId": "463058a0-4a86-4aa1-b191-dd24eb27442d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ STEP 1: Preprocessing pipeline created.\n",
            "✓ STEP 2: Stratified split done. Train size: 16512, Test size: 4128\n",
            "✓ STEP 3: 4 baseline model pipelines defined.\n",
            "✓ STEP 4: MLflow configured.\n",
            "\n",
            "================================================================================\n",
            "Training baseline model: ridge\n",
            "================================================================================\n",
            "ridge (no PCA) CV MAE: $51,151.03\n",
            "ridge (no PCA) Test MAE: $52,350.18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  warnings.warn(\n",
            "Registered model 'ridge_pipeline' already exists. Creating a new version of this model...\n",
            "Created version '5' of model 'ridge_pipeline'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Training baseline model: histgradientboosting\n",
            "================================================================================\n",
            "histgradientboosting (no PCA) CV MAE: $30,702.81\n",
            "histgradientboosting (no PCA) Test MAE: $30,702.41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  warnings.warn(\n",
            "Registered model 'histgradientboosting_pipeline' already exists. Creating a new version of this model...\n",
            "Created version '5' of model 'histgradientboosting_pipeline'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Training baseline model: xgboost\n",
            "================================================================================\n",
            "xgboost (no PCA) CV MAE: $28,831.45\n",
            "xgboost (no PCA) Test MAE: $28,465.49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  warnings.warn(\n",
            "Registered model 'xgboost_pipeline' already exists. Creating a new version of this model...\n",
            "Created version '5' of model 'xgboost_pipeline'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Training baseline model: lightgbm\n",
            "================================================================================\n",
            "lightgbm (no PCA) CV MAE: $29,422.96\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003784 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4651\n",
            "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 23\n",
            "[LightGBM] [Info] Start training from score 206333.518653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lightgbm (no PCA) Test MAE: $29,684.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "Registered model 'lightgbm_pipeline' already exists. Creating a new version of this model...\n",
            "Created version '5' of model 'lightgbm_pipeline'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ STEP 5: All 4 baseline models trained and logged.\n",
            "\n",
            "================================================================================\n",
            "Training PCA-augmented model: ridge\n",
            "================================================================================\n",
            "ridge_with_pca CV MAE: $56,480.26\n",
            "ridge_with_pca Test MAE: $56,738.52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  warnings.warn(\n",
            "Registered model 'ridge_pipeline_with_pca' already exists. Creating a new version of this model...\n",
            "Created version '4' of model 'ridge_pipeline_with_pca'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Training PCA-augmented model: histgradientboosting\n",
            "================================================================================\n",
            "histgradientboosting_with_pca CV MAE: $38,567.13\n",
            "histgradientboosting_with_pca Test MAE: $37,990.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  warnings.warn(\n",
            "Registered model 'histgradientboosting_pipeline_with_pca' already exists. Creating a new version of this model...\n",
            "Created version '4' of model 'histgradientboosting_pipeline_with_pca'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Training PCA-augmented model: xgboost\n",
            "================================================================================\n",
            "xgboost_with_pca CV MAE: $38,057.22\n",
            "xgboost_with_pca Test MAE: $37,367.20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  warnings.warn(\n",
            "Registered model 'xgboost_pipeline_with_pca' already exists. Creating a new version of this model...\n",
            "Created version '4' of model 'xgboost_pipeline_with_pca'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Training PCA-augmented model: lightgbm\n",
            "================================================================================\n",
            "lightgbm_with_pca CV MAE: $37,690.64\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015235 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 206333.518653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lightgbm_with_pca Test MAE: $37,631.80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "Registered model 'lightgbm_pipeline_with_pca' already exists. Creating a new version of this model...\n",
            "Created version '4' of model 'lightgbm_pipeline_with_pca'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ STEP 7: All 4 PCA models trained and logged.\n",
            "\n",
            "================================================================================\n",
            "GLOBAL BEST MODEL (ACROSS 8 CANDIDATES)\n",
            "================================================================================\n",
            "Global best model key: xgboost\n",
            "Global best CV MAE:    $28,831.45\n",
            "Global best Test MAE:  $28,465.49\n",
            "Uses PCA:               False\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Saving and reloading GLOBAL best model...\n",
            "--------------------------------------------------------------------------------\n",
            "✓ Model saved to /content/drive/MyDrive/Colab Notebooks/housing_fall2025/models/global_best_model.pkl\n",
            "\n",
            "Done:\n",
            "- GLOBAL best model key: xgboost\n",
            "- GLOBAL best CV MAE:    $28,831.45\n",
            "- GLOBAL best Test MAE:  $28,465.49\n",
            "Elapsed time: 2 minutes and 50.34 seconds\n"
          ]
        }
      ]
    }
  ]
}